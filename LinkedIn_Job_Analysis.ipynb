{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNGvJucx2eUx04seN2DuUbz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arausi450/Personal_Project/blob/main/LinkedIn_Job_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Decoding the Digital Job Market: An In-Depth Analysis of LinkedIn Postings**"
      ],
      "metadata": {
        "id": "f6ZeWBO44Tao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------------------------\n",
        "# LinkedIn Job Postings Analysis\n",
        "# Author: David ARAUSI\n",
        "# Date: 2024-08-25\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "# STAGE 1: DATA INGESTION\n",
        "\n",
        "# Objective: Load the dataset into the Google Colab environment.\n",
        "\n",
        "# Data Source: Kaggle - 'https://www.kaggle.com/datasets/arshkon/linkedin-job-postings'\n",
        "\n",
        "# Import the 'files' module for file uploads in Colab.\n",
        "from google.colab import files\n",
        "\n",
        "# Prompt the user to upload the 'job_postings.zip' file.\n",
        "# This code is specific to Google Colab and requires user interaction to upload the file.\n",
        "# In a different environment, data loading would be handled differently (e.g., reading from a local path or cloud storage).\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "etc1UXw_DgfV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STAGE 2: ENVIRONMENT SETUP & DATA EXTRACTION\n",
        "\n",
        "# Objective: Install required libraries and extract data from the zip archive.\n",
        "\n",
        "# Install necessary Python libraries.\n",
        "# wordcloud: For creating word clouds.\n",
        "# matplotlib & seaborn: For static data visualization.\n",
        "# plotly: For interactive visualizations.\n",
        "# nltk: For text processing.\n",
        "# scikit-learn: For machine learning tasks.\n",
        "# All necessary libraries are already listed and installed, this is good.\n",
        "!pip install wordcloud matplotlib seaborn plotly nltk scikit-learn\n",
        "\n",
        "# Import required libraries and modules.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import zipfile\n",
        "import os\n",
        "from datetime import datetime\n",
        "import re\n",
        "\n",
        "# Import specific modules for NLP and ML.\n",
        "from wordcloud import WordCloud\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation, PCA\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import nltk\n",
        "\n",
        "# Download necessary NLTK data packages only if they haven't been downloaded.\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "try:\n",
        "    nltk.data.find('corpora/wordnet')\n",
        "except LookupError:\n",
        "    nltk.download('wordnet')\n",
        "\n",
        "\n",
        "# Extract data from the zip file.\n",
        "# The extraction path is well-defined.\n",
        "with zipfile.ZipFile('job_postings.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('job_postings_data')\n",
        "\n",
        "# Verify the extracted file structure.\n",
        "# This verification step is helpful for confirming the data is extracted correctly.\n",
        "base_path = 'job_postings_data'\n",
        "for root, dirs, files in os.walk(base_path):\n",
        "    level = root.replace(base_path, '').count(os.sep)\n",
        "    indent = ' ' * 2 * level\n",
        "    print(f'{indent}{os.path.basename(root)}/')\n",
        "    subindent = ' ' * 2 * (level + 1)\n",
        "    for file in files:\n",
        "        print(f'{subindent}{file}')"
      ],
      "metadata": {
        "id": "a_EL84H2Ee2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STAGE 3: DATA LOADING\n",
        "\n",
        "# Objective: Load all raw CSV files into pandas DataFrames.\n",
        "\n",
        "# Define the base path to the extracted data.\n",
        "base_path = 'job_postings_data'\n",
        "\n",
        "# Load each CSV file into a DataFrame.\n",
        "# low_memory=False is used for safer handling of large files.\n",
        "\n",
        "# Load core job posting data.\n",
        "postings = pd.read_csv(f'{base_path}/postings.csv', low_memory=False)\n",
        "\n",
        "# Load company-related tables.\n",
        "companies = pd.read_csv(f'{base_path}/companies/companies.csv', low_memory=False)\n",
        "company_industries = pd.read_csv(f'{base_path}/companies/company_industries.csv', low_memory=False)\n",
        "company_specialities = pd.read_csv(f'{base_path}/companies/company_specialities.csv', low_memory=False)\n",
        "employee_counts = pd.read_csv(f'{base_path}/companies/employee_counts.csv', low_memory=False)\n",
        "\n",
        "# Load job-specific attribute tables.\n",
        "# These tables provide details about job skills, industries, salaries, and benefits.\n",
        "skills = pd.read_csv(f'{base_path}/mappings/skills.csv', low_memory=False)\n",
        "industries = pd.read_csv(f'{base_path}/mappings/industries.csv', low_memory=False)\n",
        "job_skills = pd.read_csv(f'{base_path}/jobs/job_skills.csv', low_memory=False)\n",
        "job_industries = pd.read_csv(f'{base_path}/jobs/job_industries.csv', low_memory=False)\n",
        "salaries = pd.read_csv(f'{base_path}/jobs/salaries.csv', low_memory=False)\n",
        "benefits = pd.read_csv(f'{base_path}/jobs/benefits.csv', low_memory=False)\n",
        "\n",
        "print(\"All raw data files loaded into DataFrames.\")"
      ],
      "metadata": {
        "id": "Lg8zY-4QGIB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STAGE 4: INITIAL DATA EXPLORATION & PROFILING\n",
        "\n",
        "# Objective: Inspect each DataFrame's structure, size, and missing values.\n",
        "\n",
        "# Helper function to print key summary statistics for a given DataFrame.\n",
        "def dataset_info(df, name):\n",
        "    \"\"\"\n",
        "    Prints the shape, columns, and missing value counts for a given DataFrame.\n",
        "    This is a good practice for quickly understanding the structure and completeness\n",
        "    of each dataset.\n",
        "    \"\"\"\n",
        "    print(f\"=== {name} ===\")\n",
        "    print(f\"Shape: {df.shape}\")\n",
        "    print(f\"Columns: {list(df.columns)}\")\n",
        "    print(f\"Missing values:\\n{df.isnull().sum()}\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "# Collect all loaded DataFrames in a dictionary for easy iteration.\n",
        "datasets = {\n",
        "    \"Postings\": postings,\n",
        "    \"Companies\": companies,\n",
        "    \"Skills\": skills,\n",
        "    \"Industries\": industries,\n",
        "    \"Job Skills\": job_skills,\n",
        "    \"Job Industries\": job_industries,\n",
        "    \"Salaries\": salaries,\n",
        "    \"Benefits\": benefits,\n",
        "    \"Company Industries\": company_industries,\n",
        "    \"Company Specialities\": company_specialities,\n",
        "    \"Employee Counts\": employee_counts\n",
        "}\n",
        "\n",
        "# Iterate through the datasets and print info using the helper function.\n",
        "for name, df in datasets.items():\n",
        "    dataset_info(df, name)"
      ],
      "metadata": {
        "id": "rWFhrtJQGm_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STAGE 5: DEEP DATA INSPECTION & QUALITY ASSESSMENT\n",
        "\n",
        "# Objective: Conduct detailed data quality checks before cleaning and merging.\n",
        "\n",
        "# Identify potential join keys by looking for common column names.\n",
        "# This helps understand how the different tables can be linked.\n",
        "print(\"=== KEY COLUMNS FOR JOINING DATASETS ===\")\n",
        "print(\"Postings columns:\", [col for col in postings.columns if 'id' in col.lower() or 'job' in col.lower()])\n",
        "print(\"Companies columns:\", [col for col in companies.columns if 'id' in col.lower() or 'company' in col.lower()])\n",
        "print(\"Skills columns:\", [col for col in skills.columns if 'id' in col.lower() or 'skill' in col.lower()])\n",
        "print(\"Industries columns:\", [col for col in industries.columns if 'id' in col.lower() or 'industry' in col.lower()])\n",
        "print(\"Job Skills columns:\", [col for col in job_skills.columns if 'id' in col.lower() or 'job' in col.lower() or 'skill' in col.lower()])\n",
        "print(\"Job Industries columns:\", [col for col in job_industries.columns if 'id' in col.lower() or 'job' in col.lower() or 'industry' in col.lower()])\n",
        "print(\"Company Industries columns:\", [col for col in company_industries.columns if 'id' in col.lower() or 'company' in col.lower() or 'industry' in col.lower()])\n",
        "print(\"Company Specialities columns:\", [col for col in company_specialities.columns if 'id' in col.lower() or 'company' in col.lower() or 'specialit' in col.lower()])\n",
        "print(\"Employee Counts columns:\", [col for col in employee_counts.columns if 'id' in col.lower() or 'company' in col.lower() or 'employee' in col.lower()])\n",
        "\n",
        "# Check for duplicate records in each dataset.\n",
        "# Essential for ensuring data integrity before merging.\n",
        "print(\"\\n=== DUPLICATE CHECK ===\")\n",
        "for name, df in datasets.items():\n",
        "    duplicates = df.duplicated().sum()\n",
        "    print(f\"{name}: {duplicates} duplicates\")\n",
        "\n",
        "# Verify data types for each column.\n",
        "# Crucial for correct data manipulation and analysis.\n",
        "print(\"\\n=== DATA TYPES ===\")\n",
        "for name, df in datasets.items():\n",
        "    print(f\"\\n--- {name} ---\")\n",
        "    print(df.dtypes)"
      ],
      "metadata": {
        "id": "82fzaqr0HG1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STAGE 6: DATA CLEANING AND PREPROCESSING - Part 1\n",
        "\n",
        "# Objective: Clean and preprocess the dataframes before merging.\n",
        "\n",
        "print(\"=== DATA TYPE CORRECTIONS ===\")\n",
        "\n",
        "# Convert 'company_id' in postings to integer, handling NaNs.\n",
        "# This conversion is necessary for merging with other tables on company_id.\n",
        "print(f\"Postings company_id NaN count: {postings['company_id'].isna().sum()}\")\n",
        "# Creating a boolean column to track missing company_ids before filling NaNs.\n",
        "postings['company_id_missing'] = postings['company_id'].isna()\n",
        "# Filling NaN company_ids with -1 and converting to integer.\n",
        "postings['company_id'] = postings['company_id'].fillna(-1).astype(int)\n",
        "\n",
        "# Drop rows with missing 'industry_name' in the industries table.\n",
        "# Rows with missing industry names are not useful for analysis.\n",
        "print(f\"Industries with missing names: {industries['industry_name'].isna().sum()}\")\n",
        "industries = industries.dropna(subset=['industry_name'])\n",
        "\n",
        "# Convert Unix timestamp columns in postings to datetime objects.\n",
        "# Converting timestamps to datetime objects is crucial for time-series analysis.\n",
        "date_columns = ['original_listed_time', 'listed_time', 'expiry', 'closed_time']\n",
        "for col in date_columns:\n",
        "    if col in postings.columns:\n",
        "        # Using errors='coerce' will turn unparseable dates into NaT (Not a Time).\n",
        "        postings[col] = pd.to_datetime(postings[col], unit='s', errors='coerce')\n",
        "        print(f\"Converted {col} to datetime\")\n",
        "\n",
        "print(\"\\n=== HANDLING MISSING VALUES ===\")\n",
        "\n",
        "# Fill missing values in 'description' and 'company_name' columns.\n",
        "# Filling missing text data with placeholders prevents errors in text processing.\n",
        "postings['description'] = postings['description'].fillna('No description available')\n",
        "postings['company_name'] = postings['company_name'].fillna('Unknown Company')"
      ],
      "metadata": {
        "id": "zWqKkXBAHgC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STAGE 6: DATA CLEANING AND PREPROCESSING - Part 2\n",
        "\n",
        "# Objective: Create a comprehensive company dataset by merging related tables.\n",
        "\n",
        "print(\"\\n=== CREATING COMPREHENSIVE COMPANY DATASET ===\")\n",
        "\n",
        "# Map industry names to industry IDs for merging.\n",
        "industry_name_to_id = dict(zip(industries['industry_name'], industries['industry_id']))\n",
        "\n",
        "def map_industry_name_to_id(industry_name):\n",
        "    \"\"\"Maps an industry name to its ID, returning -1 if not found.\"\"\"\n",
        "    # Ensure the industry_name is not NaN before mapping.\n",
        "    if pd.isna(industry_name):\n",
        "        return -1\n",
        "    return industry_name_to_id.get(industry_name, -1)\n",
        "\n",
        "# Apply the mapping to the 'industry' column in company_industries.\n",
        "company_industries['industry_id'] = company_industries['industry'].apply(map_industry_name_to_id)\n",
        "\n",
        "# Merge company-related DataFrames.\n",
        "# Merge with company_industries on company_id and industry_id.\n",
        "companies_full = companies.merge(\n",
        "    company_industries[['company_id', 'industry_id']].drop_duplicates(),\n",
        "    on='company_id',\n",
        "    how='left'\n",
        ").merge(\n",
        "    # Aggregate company specialities by company_id.\n",
        "    company_specialities.groupby('company_id')['speciality'].apply(lambda x: ', '.join(x)).reset_index(name='speciality'),\n",
        "    on='company_id',\n",
        "    how='left'\n",
        ").merge(\n",
        "    # Aggregate employee and follower counts by company_id.\n",
        "    employee_counts.groupby('company_id').agg({\n",
        "        'employee_count': 'mean',\n",
        "        'follower_count': 'mean',\n",
        "        'time_recorded': 'max' # Use the latest time recorded\n",
        "    }).reset_index(),\n",
        "    on='company_id',\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "print(f\"Comprehensive company dataset shape: {companies_full.shape}\")"
      ],
      "metadata": {
        "id": "2mHtUVZwIGEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STAGE 6: DATA CLEANING AND PREPROCESSING - Part 3\n",
        "\n",
        "# Objective: Create a comprehensive job postings dataset by merging relevant tables.\n",
        "\n",
        "print(\"\\n=== CREATING COMPREHENSIVE JOB POSTINGS DATASET ===\")\n",
        "\n",
        "# Aggregate Job Skills.\n",
        "job_skills_full = job_skills.merge(skills, on='skill_abr', how='left')\n",
        "# Join skills associated with each job into a single string.\n",
        "skills_by_job = job_skills_full.groupby('job_id')['skill_name'].apply(lambda x: ', '.join(x.dropna())).reset_index()\n",
        "skills_by_job.columns = ['job_id', 'skills_list']\n",
        "\n",
        "# Aggregate Job Industries.\n",
        "job_industries_full = job_industries.merge(industries, on='industry_id', how='left')\n",
        "# Join industries associated with each job into a single string.\n",
        "industries_by_job = job_industries_full.groupby('job_id')['industry_name'].apply(lambda x: ', '.join(x.dropna())).reset_index()\n",
        "industries_by_job.columns = ['job_id', 'industries_list']\n",
        "\n",
        "# Aggregate Job Benefits.\n",
        "# Join benefits associated with each job into a single string.\n",
        "benefits_by_job = benefits.groupby('job_id')['type'].apply(lambda x: ', '.join(x.dropna())).reset_index()\n",
        "benefits_by_job.columns = ['job_id', 'benefits_list']\n",
        "\n",
        "# Merge postings with aggregated data.\n",
        "# Perform a series of left merges to combine job postings with skills, industries, salaries, benefits, and company information.\n",
        "postings_full = postings.merge(\n",
        "    skills_by_job,\n",
        "    on='job_id',\n",
        "    how='left'\n",
        ").merge(\n",
        "    industries_by_job,\n",
        "    on='job_id',\n",
        "    how='left'\n",
        ").merge(\n",
        "    # Use salaries table for complete salary info, adding a suffix to distinguish from original postings columns.\n",
        "    salaries[['job_id', 'max_salary', 'min_salary', 'med_salary', 'pay_period', 'currency', 'compensation_type']],\n",
        "    on='job_id',\n",
        "    how='left',\n",
        "    suffixes=('', '_salary')\n",
        ").merge(\n",
        "    benefits_by_job,\n",
        "    on='job_id',\n",
        "    how='left'\n",
        ").merge(\n",
        "    companies_full,\n",
        "    on='company_id',\n",
        "    how='left',\n",
        "    # Add suffixes to distinguish columns originating from the job postings and company dataframes.\n",
        "    suffixes=('_job', '_company')\n",
        ")\n",
        "\n",
        "print(f\"Comprehensive postings dataset shape: {postings_full.shape}\")\n",
        "print(f\"Comprehensive postings dataset columns: {list(postings_full.columns)}\")\n",
        "\n",
        "# --- Handling Missing Values and Standardizing Salary (Moved from STAGE 7) ---\n",
        "\n",
        "print(\"\\n=== HANDLING MISSING VALUES AND STANDARDIZING SALARY ===\")\n",
        "\n",
        "# Fill missing 'skills_list' and 'industries_list'.\n",
        "# Filling with 'Not specified' provides a category for these missing values.\n",
        "postings_full['skills_list'] = postings_full['skills_list'].fillna('Not specified')\n",
        "postings_full['industries_list'] = postings_full['industries_list'].fillna('Not specified')\n",
        "print(\"Filled missing values in 'skills_list' and 'industries_list'.\")\n",
        "\n",
        "# Standardize Salary Information.\n",
        "# Create a calculated average salary from min and max salary columns.\n",
        "postings_full['calculated_avg_salary'] = postings_full[['min_salary_salary', 'max_salary_salary']].mean(axis=1)\n",
        "\n",
        "# Function to convert salaries to annual equivalent.\n",
        "def standardize_salary(row):\n",
        "    \"\"\"Converts salaries to an annual equivalent based on pay period.\"\"\"\n",
        "    salary = row['calculated_avg_salary']\n",
        "    pay_period = row['pay_period_salary'] # Use the pay_period from salaries table\n",
        "\n",
        "    # Return None for rows with missing salary or pay period.\n",
        "    if pd.isna(salary) or pd.isna(pay_period):\n",
        "        return None\n",
        "\n",
        "    # Convert salary based on pay period.\n",
        "    if pay_period == 'HOURLY':\n",
        "        # Assuming a 40-hour work week and 52 weeks a year.\n",
        "        return salary * 40 * 52\n",
        "    elif pay_period == 'MONTHLY':\n",
        "        return salary * 12\n",
        "    elif pay_period == 'YEARLY':\n",
        "        return salary\n",
        "    else:\n",
        "        # Return None for unknown pay periods.\n",
        "        return None\n",
        "\n",
        "# Apply function to create standardized annual salary.\n",
        "postings_full['annual_salary'] = postings_full.apply(standardize_salary, axis=1)\n",
        "\n",
        "print(\"\\n--- Post-Handling Data Quality Check ---\")\n",
        "print(f\"Missing values in 'skills_list' after handling: {postings_full['skills_list'].isna().sum()}\")\n",
        "print(f\"Missing values in 'industries_list' after handling: {postings_full['industries_list'].isna().sum()}\")\n",
        "non_null_annual = postings_full['annual_salary'].notna().sum()\n",
        "print(f\"Non-null values in 'annual_salary': {non_null_annual} ({non_null_annual/len(postings_full)*100:.2f}%)\")\n",
        "\n",
        "print(\"\\nData cleaning and preprocessing complete!\")"
      ],
      "metadata": {
        "id": "w3s-DxNbIal3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STAGE 7: FINAL DATA QUALITY CHECK\n",
        "\n",
        "# Objective: Perform a final check of the merged dataset for missing values in key columns.\n",
        "\n",
        "print(\"\\n=== FINAL DATA QUALITY CHECK ===\")\n",
        "\n",
        "# Check missing values in key text columns.\n",
        "# It's important to see how many missing values remain after merging and filling.\n",
        "print(\"Missing values in key columns:\")\n",
        "key_columns = ['title', 'description_job', 'company_name', 'skills_list', 'industries_list']\n",
        "for col in key_columns:\n",
        "    if col in postings_full.columns:\n",
        "        missing = postings_full[col].isna().sum()\n",
        "        print(f\"{col}: {missing} missing values ({missing/len(postings_full)*100:.2f}%)\")\n",
        "\n",
        "# Check non-null values for salary columns.\n",
        "# Assessing the completeness of salary data is crucial for salary analysis.\n",
        "print(\"\\nNon-null salary values:\")\n",
        "salary_cols = ['max_salary_salary', 'min_salary_salary', 'med_salary_salary']\n",
        "for col in salary_cols:\n",
        "    if col in postings_full.columns:\n",
        "        non_null = postings_full[col].notna().sum()\n",
        "        print(f\"{col}: {non_null} non-null values ({non_null/len(postings_full)*100:.2f}%)\")\n",
        "\n",
        "print(\"\\nData cleaning and preprocessing complete!\")"
      ],
      "metadata": {
        "id": "10R1nWFsI2L1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exploratory Data Analysis (EDA) - Part 1\n",
        "\n",
        "# Import number formatter for plot axes.\n",
        "from matplotlib.ticker import FuncFormatter\n",
        "\n",
        "# Set plot style.\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.figure(figsize=(18, 12))\n",
        "\n",
        "# Custom number formatter for 'k' (thousands).\n",
        "def k_formatter(x, pos):\n",
        "    \"\"\"Formats numbers to a short scale (e.g., 40,000 -> 40k)\"\"\"\n",
        "    if x >= 1000:\n",
        "        return f'{x/1000:,.0f}k'\n",
        "    return f'{x:,.0f}'\n",
        "\n",
        "# 1. Distribution of Job Postings by Experience Level.\n",
        "plt.subplot(2, 2, 1)\n",
        "exp_level_col = 'formatted_experience_level'\n",
        "if exp_level_col in postings_full.columns:\n",
        "    plot_data = postings_full.dropna(subset=[exp_level_col])\n",
        "    if not plot_data.empty and plot_data[exp_level_col].nunique() > 1: # Add check for unique values\n",
        "        ax1 = sns.countplot(y=exp_level_col, data=plot_data, order=plot_data[exp_level_col].value_counts().index, palette='viridis')\n",
        "        plt.title('Distribution of Job Postings by Experience Level', fontsize=16)\n",
        "        plt.xlabel('Number of Postings', fontsize=12)\n",
        "        plt.ylabel('Experience Level', fontsize=12)\n",
        "        ax1.xaxis.set_major_formatter(FuncFormatter(k_formatter))\n",
        "    else:\n",
        "        plt.text(0.5, 0.5, 'Not enough data for experience level plot.', ha='center')\n",
        "        plt.title('Experience Level Data Not Available')\n",
        "else:\n",
        "    plt.text(0.5, 0.5, 'Experience Level column not found.', ha='center')\n",
        "    plt.title('Experience Level Data Not Available')\n",
        "\n",
        "# 2. Top 10 Most Common Job Titles.\n",
        "plt.subplot(2, 2, 2)\n",
        "top_10_titles = postings_full['title'].value_counts().nlargest(10)\n",
        "if not top_10_titles.empty and len(top_10_titles) > 1: # Add check for number of values\n",
        "    ax2 = sns.barplot(x=top_10_titles.values, y=top_10_titles.index, palette='plasma')\n",
        "    plt.title('Top 10 Most Common Job Titles', fontsize=16)\n",
        "    plt.xlabel('Number of Postings', fontsize=12)\n",
        "    plt.ylabel('Job Title', fontsize=12)\n",
        "    ax2.xaxis.set_major_formatter(FuncFormatter(k_formatter))\n",
        "else:\n",
        "    plt.text(0.5, 0.5, 'Not enough data for job titles plot.', ha='center')\n",
        "    plt.title('Job Title Data Not Available')\n",
        "\n",
        "\n",
        "# 3. Distribution of Job Postings by Employment Type.\n",
        "plt.subplot(2, 2, 3)\n",
        "emp_type_col = 'formatted_work_type'\n",
        "if emp_type_col in postings_full.columns:\n",
        "    plot_data_emp = postings_full.dropna(subset=[emp_type_col])\n",
        "    if not plot_data_emp.empty and plot_data_emp[emp_type_col].nunique() > 1: # Add check for unique values\n",
        "        ax3 = sns.countplot(y=emp_type_col, data=plot_data_emp, order=plot_data_emp[emp_type_col].value_counts().index, palette='magma')\n",
        "        plt.title('Distribution of Job Postings by Employment Type', fontsize=16)\n",
        "        plt.xlabel('Number of Postings', fontsize=12)\n",
        "        plt.ylabel('Employment Type', fontsize=12)\n",
        "        ax3.xaxis.set_major_formatter(FuncFormatter(k_formatter))\n",
        "    else:\n",
        "        plt.text(0.5, 0.5, 'Not enough data for employment type plot.', ha='center')\n",
        "        plt.title('Employment Type Data Not Available')\n",
        "else:\n",
        "    plt.text(0.5, 0.5, 'Employment Type column not found.', ha='center')\n",
        "    plt.title('Employment Type Data Not Available')\n",
        "\n",
        "# 4. Distribution of On-site vs. Remote Jobs (Donut Chart).\n",
        "plt.subplot(2, 2, 4)\n",
        "remote_col = 'remote_allowed'\n",
        "if remote_col in postings_full.columns:\n",
        "    remote_labels = postings_full[remote_col].apply(lambda x: 'Remote Allowed' if x == 1.0 else 'Not Remote')\n",
        "    remote_counts = remote_labels.value_counts()\n",
        "\n",
        "    if not remote_counts.empty and len(remote_counts) > 1: # Add check for number of values\n",
        "        plt.pie(remote_counts, labels=remote_counts.index, autopct='%1.1f%%',\n",
        "                colors=sns.color_palette('coolwarm'), startangle=90,\n",
        "                textprops={'fontsize': 12}, pctdistance=1.1, labeldistance=1.25)\n",
        "\n",
        "        centre_circle = plt.Circle((0, 0), 0.60, fc='white')\n",
        "        fig = plt.gcf()\n",
        "        fig.gca().add_artist(centre_circle)\n",
        "\n",
        "        plt.title('On-site vs. Remote Job Postings', fontsize=16)\n",
        "    else:\n",
        "        plt.text(0.5, 0.5, 'Not enough data for remote vs. on-site plot.', ha='center')\n",
        "        plt.title('On-site vs. Remote Data Not Available')\n",
        "else:\n",
        "    plt.text(0.5, 0.5, 'Remote column not found.', ha='center')\n",
        "    plt.title('Remote Data Not Available')\n",
        "\n",
        "# Add main title.\n",
        "plt.suptitle('Overview of LinkedIn Job Postings', fontsize=22, y=1.02)\n",
        "plt.tight_layout(pad=3.0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lPN_8JhXJ39t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exploratory Data Analysis (EDA) - Part 2: Industries and Skills\n",
        "\n",
        "# Set plot style.\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.figure(figsize=(18, 8))\n",
        "\n",
        "# 1. Top 10 Industries with the Most Job Postings.\n",
        "plt.subplot(1, 2, 1)\n",
        "top_industries = postings_full['industries_list'].str.split(', ').explode().value_counts().nlargest(10)\n",
        "# Add check for empty series before plotting.\n",
        "if not top_industries.empty and len(top_industries) > 1:\n",
        "    sns.barplot(x=top_industries.values, y=top_industries.index, palette='crest')\n",
        "    plt.title('Top 10 Industries by Number of Job Postings', fontsize=16)\n",
        "    plt.xlabel('Number of Postings', fontsize=12)\n",
        "    plt.ylabel('Industry', fontsize=12)\n",
        "    plt.gca().xaxis.set_major_formatter(FuncFormatter(k_formatter))\n",
        "else:\n",
        "    plt.text(0.5, 0.5, 'Not enough data for top industries plot.', ha='center')\n",
        "    plt.title('Top 10 Industries Data Not Available')\n",
        "\n",
        "\n",
        "# 2. Word Cloud of Most In-Demand Skills.\n",
        "plt.subplot(1, 2, 2)\n",
        "# Ensure there's data for the word cloud.\n",
        "skills_text = ' '.join(postings_full['skills_list'].dropna())\n",
        "if skills_text: # Check if the string is not empty\n",
        "    # Generate word cloud.\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white', colormap='viridis').generate(skills_text)\n",
        "\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    plt.title('Most In-Demand Skills', fontsize=16)\n",
        "else:\n",
        "    plt.text(0.5, 0.5, 'No skill data for word cloud.', ha='center')\n",
        "    plt.title('Most In-Demand Skills Data Not Available')\n",
        "\n",
        "\n",
        "# Adjust layout and add main title.\n",
        "plt.suptitle('Job Market Insights: Top Industries and Skills', fontsize=22, y=1.05)\n",
        "plt.tight_layout(pad=3.0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AMc0nPgTPIo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exploratory Data Analysis (EDA) - Part 3: Salary Analysis\n",
        "\n",
        "# Set plot style.\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.figure(figsize=(18, 8))\n",
        "\n",
        "# 1. Distribution of Annual Salaries.\n",
        "plt.subplot(1, 2, 1)\n",
        "# Cap salary at 95th percentile for better visualization.\n",
        "salary_data = postings_full['annual_salary'].dropna()\n",
        "if not salary_data.empty: # Add check for empty series\n",
        "    quantile_95 = salary_data.quantile(0.95)\n",
        "    sns.histplot(salary_data[salary_data < quantile_95], bins=30, kde=True, color='skyblue')\n",
        "    plt.title('Distribution of Annual Salaries (up to 95th percentile)', fontsize=16)\n",
        "    plt.xlabel('Annual Salary (USD)', fontsize=12)\n",
        "    plt.ylabel('Number of Postings', fontsize=12)\n",
        "    plt.gca().xaxis.set_major_formatter(FuncFormatter(lambda x, pos: f'${x/1000:,.0f}k'))\n",
        "else:\n",
        "    plt.text(0.5, 0.5, 'No salary data for distribution plot.', ha='center')\n",
        "    plt.title('Salary Distribution Data Not Available')\n",
        "\n",
        "# 2. Median Salary by Experience Level.\n",
        "plt.subplot(1, 2, 2)\n",
        "median_salary_exp = postings_full.groupby('formatted_experience_level')['annual_salary'].median().dropna().sort_values()\n",
        "if not median_salary_exp.empty and len(median_salary_exp) > 1: # Add check for number of values\n",
        "    sns.barplot(x=median_salary_exp.values, y=median_salary_exp.index, palette='rocket')\n",
        "    plt.title('Median Annual Salary by Experience Level', fontsize=16)\n",
        "    plt.xlabel('Median Annual Salary (USD)', fontsize=12)\n",
        "    plt.ylabel('Experience Level', fontsize=12)\n",
        "    plt.gca().xaxis.set_major_formatter(FuncFormatter(lambda y, pos: f'${y:,.0f}')) # Changed formatter to lambda y, pos\n",
        "else:\n",
        "    plt.text(0.5, 0.5, 'Not enough data for median salary by experience level plot.', ha='center')\n",
        "    plt.title('Median Salary by Experience Level Data Not Available')\n",
        "\n",
        "\n",
        "# Adjust layout and add main title.\n",
        "plt.suptitle('Job Market Insights: Salary Analysis', fontsize=22, y=1.05)\n",
        "plt.tight_layout(pad=3.0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CADnsqmuPwYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exploratory Data Analysis (EDA) - Part 4: Company Analysis\n",
        "\n",
        "# Set plot style.\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.figure(figsize=(18, 8))\n",
        "\n",
        "# 1. Top 10 Companies with the Most Job Postings.\n",
        "plt.subplot(1, 2, 1)\n",
        "known_companies = postings_full[postings_full['company_name'] != 'Unknown Company']\n",
        "top_companies = known_companies['company_name'].value_counts().nlargest(10)\n",
        "sns.barplot(x=top_companies.values, y=top_companies.index, palette='mako')\n",
        "plt.title('Top 10 Companies by Number of Job Postings', fontsize=16)\n",
        "plt.xlabel('Number of Postings', fontsize=12)\n",
        "plt.ylabel('Company Name', fontsize=12)\n",
        "plt.gca().xaxis.set_major_formatter(FuncFormatter(k_formatter))\n",
        "\n",
        "# 2. Median Salary by Company Size.\n",
        "plt.subplot(1, 2, 2)\n",
        "# Categorize company size.\n",
        "bins = [0, 50, 200, 1000, 5000, 10000, float('inf')]\n",
        "labels = ['1-50', '51-200', '201-1k', '1k-5k', '5k-10k', '10k+']\n",
        "postings_full['company_size_category'] = pd.cut(postings_full['employee_count'], bins=bins, labels=labels, right=False)\n",
        "\n",
        "# Calculate median salary by company size category.\n",
        "median_salary_size = postings_full.groupby('company_size_category')['annual_salary'].median().dropna()\n",
        "sns.barplot(x=median_salary_size.index, y=median_salary_size.values, palette='viridis')\n",
        "plt.title('Median Annual Salary by Company Size', fontsize=16)\n",
        "plt.xlabel('Company Size (Number of Employees)', fontsize=12)\n",
        "plt.ylabel('Median Annual Salary (USD)', fontsize=12)\n",
        "plt.gca().yaxis.set_major_formatter(FuncFormatter(lambda y, pos: f'${y/1000:,.0f}k'))\n",
        "\n",
        "# Adjust layout and add main title.\n",
        "plt.suptitle('Job Market Insights: Company Analysis', fontsize=22, y=1.05)\n",
        "plt.tight_layout(pad=3.0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kkrYOHzTQr7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exploratory Data Analysis (EDA) - Part 5: Geographical Analysis\n",
        "\n",
        "# Set plot style.\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Top 10 Locations with the Most Job Postings.\n",
        "# Map US state abbreviations to full names.\n",
        "state_map = {\n",
        "    'AL': 'Alabama', 'AK': 'Alaska', 'AZ': 'Arizona', 'AR': 'Arkansas', 'CA': 'California',\n",
        "    'CO': 'Colorado', 'CT': 'Connecticut', 'DE': 'Delaware', 'FL': 'Florida', 'GA': 'Georgia',\n",
        "    'HI': 'Hawaii', 'ID': 'Idaho', 'IL': 'Illinois', 'IN': 'Indiana', 'IA': 'Iowa',\n",
        "    'KS': 'Kansas', 'KY': 'Kentucky', 'LA': 'Louisiana', 'ME': 'Maine', 'MD': 'Maryland',\n",
        "    'MA': 'Massachusetts', 'MI': 'Michigan', 'MN': 'Minnesota', 'MS': 'Mississippi',\n",
        "    'MO': 'Missouri', 'MT': 'Montana', 'NE': 'Nebraska', 'NV': 'Nevada', 'NH': 'New Hampshire',\n",
        "    'NJ': 'New Jersey', 'NM': 'New Mexico', 'NY': 'New York', 'NC': 'North Carolina',\n",
        "    'ND': 'North Dakota', 'OH': 'Ohio', 'OK': 'Oklahoma', 'OR': 'Oregon', 'PA': 'Pennsylvania',\n",
        "    'RI': 'Rhode Island', 'SC': 'South Carolina', 'SD': 'South Dakota', 'TN': 'Tennessee',\n",
        "    'TX': 'Texas', 'UT': 'Utah', 'VT': 'Vermont', 'VA': 'Virginia', 'WA': 'Washington',\n",
        "    'WV': 'West Virginia', 'WI': 'Wisconsin', 'WY': 'Wyoming',\n",
        "    'DC': 'District of Columbia', 'PR': 'Puerto Rico'\n",
        "}\n",
        "\n",
        "# Extract state from location and filter US states.\n",
        "postings_full['state'] = postings_full['location'].str.split(', ').str[-1]\n",
        "us_states_only = postings_full[postings_full['state'].isin(state_map.keys())] # Filter using keys\n",
        "\n",
        "# Count job postings by state.\n",
        "top_locations = us_states_only['state'].value_counts().nlargest(10)\n",
        "\n",
        "# Map abbreviations to full names for plotting.\n",
        "top_locations.index = [state_map.get(abbr, abbr) for abbr in top_locations.index]\n",
        "\n",
        "sns.barplot(x=top_locations.values, y=top_locations.index, palette='cubehelix')\n",
        "plt.title('Top 10 States by Number of Job Postings', fontsize=16)\n",
        "plt.xlabel('Number of Postings', fontsize=12)\n",
        "plt.ylabel('State', fontsize=12)\n",
        "plt.gca().xaxis.set_major_formatter(FuncFormatter(k_formatter))\n",
        "\n",
        "# Adjust layout and add main title.\n",
        "plt.suptitle('Job Market Insights: Geographical Distribution', fontsize=22, y=1.02)\n",
        "plt.tight_layout(pad=3.0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vkw642WyRvQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Natural Language Processing (NLP) on Job Descriptions\n",
        "\n",
        "# Import necessary libraries for text processing.\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "\n",
        "# Ensure necessary NLTK data is downloaded.\n",
        "# These downloads are necessary for NLP.\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "try:\n",
        "    nltk.data.find('corpora/wordnet')\n",
        "except LookupError:\n",
        "    nltk.download('wordnet')\n",
        "\n",
        "\n",
        "# Text Preprocessing.\n",
        "# Use a sample of the data for manageable processing.\n",
        "df_sample = postings_full.sample(10000, random_state=42)\n",
        "\n",
        "# Initialize lemmatizer and stopwords.\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Cleans and prepares text for NLP.\"\"\"\n",
        "    text = str(text) # Ensure text is a string\n",
        "    text = re.sub('[^a-zA-Z]', ' ', text).lower()\n",
        "    words = text.split()\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words and len(word) > 2]\n",
        "    return ' '.join(lemmatized_words)\n",
        "\n",
        "# Apply preprocessing to job descriptions.\n",
        "df_sample['processed_description'] = df_sample['description_job'].apply(preprocess_text)\n",
        "\n",
        "# Bigram Analysis (Two-Word Phrases).\n",
        "vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
        "bigram_matrix = vectorizer.fit_transform(df_sample['processed_description'])\n",
        "bigram_counts = bigram_matrix.sum(axis=0)\n",
        "bigrams_freq = [(word, bigram_counts[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\n",
        "sorted_bigrams = sorted(bigrams_freq, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Visualization of top bigrams.\n",
        "top_20_bigrams = pd.DataFrame(sorted_bigrams[:20], columns=['bigram', 'count'])\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.barplot(x='count', y='bigram', data=top_20_bigrams, palette='inferno')\n",
        "plt.title('Top 20 Most Common Phrases in Job Descriptions', fontsize=16)\n",
        "plt.xlabel('Frequency', fontsize=12)\n",
        "plt.ylabel('Phrase', fontsize=12)\n",
        "plt.gca().xaxis.set_major_formatter(FuncFormatter(k_formatter))\n",
        "\n",
        "plt.suptitle('Job Market Insights: NLP Analysis', fontsize=22, y=1.02)\n",
        "plt.tight_layout(pad=3.0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1dNrGMblUznl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Machine Learning - Predicting Annual Salary\n",
        "\n",
        "# Import necessary libraries for machine learning.\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt # Import matplotlib for plotting\n",
        "import seaborn as sns # Import seaborn for plotting\n",
        "\n",
        "\n",
        "# Feature Selection and Data Preparation.\n",
        "features = [\n",
        "    'formatted_experience_level',\n",
        "    'company_size_category',\n",
        "    'state',\n",
        "    'remote_allowed'\n",
        "]\n",
        "target = 'annual_salary'\n",
        "\n",
        "# Create DataFrame with selected features and target, dropping NaNs.\n",
        "df_model = postings_full[features + [target]].dropna()\n",
        "\n",
        "# Identify categorical columns.\n",
        "categorical_features = df_model.select_dtypes(include=['object', 'category']).columns\n",
        "\n",
        "# Create Preprocessing and Modeling Pipeline.\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "\n",
        "# Define the model.\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "\n",
        "# Create the full pipeline.\n",
        "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                           ('regressor', model)])\n",
        "\n",
        "# Split Data, Train Model, and Evaluate.\n",
        "X = df_model[features]\n",
        "y = df_model[target]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model.\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions.\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "# Evaluate performance.\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"--- Model Performance ---\")\n",
        "print(f\"Mean Absolute Error (MAE): ${mae:,.2f}\")\n",
        "print(f\"R-squared (RÂ²): {r2:.2f}\")\n",
        "print(\"\\nModel training and evaluation complete!\")\n",
        "\n",
        "# Feature Importance.\n",
        "# Ensure feature_names are correctly obtained after fitting the preprocessor.\n",
        "feature_names = pipeline.named_steps['preprocessor'].get_feature_names_out(input_features=features)\n",
        "\n",
        "importances = pipeline.named_steps['regressor'].feature_importances_\n",
        "feature_importance_df = pd.DataFrame({'feature': feature_names, 'importance': importances}).sort_values('importance', ascending=False).head(15)\n",
        "\n",
        "# Plot important features.\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.barplot(x='importance', y='feature', data=feature_importance_df, palette='viridis')\n",
        "plt.title('Top 15 Most Important Features for Predicting Salary', fontsize=16)\n",
        "plt.xlabel('Importance', fontsize=12)\n",
        "plt.ylabel('Feature', fontsize=12)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "V6ShEbR0YAeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Time Series Analysis - Job Posting Trends Over Time\n",
        "\n",
        "# Data Preparation for Time Series.\n",
        "# Use 'listed_time' and ensure datetime format.\n",
        "df_time = postings_full[['listed_time']].dropna()\n",
        "df_time['listed_time'] = pd.to_datetime(df_time['listed_time'])\n",
        "df_time = df_time.set_index('listed_time')\n",
        "\n",
        "# Resample Data by Month.\n",
        "monthly_postings = df_time.resample('ME').size()\n",
        "\n",
        "# Visualization.\n",
        "plt.figure(figsize=(14, 7))\n",
        "\n",
        "if not monthly_postings.empty:\n",
        "    monthly_postings.plot(kind='line', marker='o', linestyle='-', color='teal')\n",
        "\n",
        "    plt.title('Monthly Job Posting Trends', fontsize=16)\n",
        "    plt.xlabel('Date', fontsize=12)\n",
        "    plt.ylabel('Number of Job Postings', fontsize=12)\n",
        "    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
        "    plt.gca().yaxis.set_major_formatter(FuncFormatter(k_formatter))\n",
        "else:\n",
        "    plt.text(0.5, 0.5, 'No valid time series data available to plot.',\n",
        "             horizontalalignment='center', verticalalignment='center',\n",
        "             fontsize=14, color='red')\n",
        "    plt.title('Monthly Job Posting Trends', fontsize=16)\n",
        "    plt.xlabel('Date', fontsize=12)\n",
        "    plt.ylabel('Number of Job Postings', fontsize=12)\n",
        "\n",
        "plt.suptitle('Job Market Insights: Time Series Analysis', fontsize=22, y=1.02)\n",
        "plt.tight_layout(pad=3.0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6Mf-UlpJZixJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unsupervised ML - Clustering Similar Job Descriptions\n",
        "\n",
        "# Import necessary libraries.\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Data Preparation.\n",
        "# Use preprocessed text from NLP analysis.\n",
        "if 'processed_description' not in df_sample.columns:\n",
        "    print(\"Preprocessing text data... please wait.\")\n",
        "    # Re-run preprocessing if needed.\n",
        "    df_sample['processed_description'] = df_sample['description_job'].apply(preprocess_text)\n",
        "\n",
        "# TF-IDF Vectorization.\n",
        "tfidf_vectorizer = TfidfVectorizer(max_df=0.8, min_df=5, stop_words='english')\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(df_sample['processed_description'])\n",
        "\n",
        "# K-Means Clustering.\n",
        "# Using a fixed number of clusters (5) for demonstration.\n",
        "# In a real-world scenario, you might use methods like the elbow method\n",
        "# or silhouette score to determine an optimal number of clusters.\n",
        "num_clusters = 5\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)\n",
        "kmeans.fit(tfidf_matrix)\n",
        "\n",
        "# Add cluster labels to DataFrame.\n",
        "df_sample['cluster'] = kmeans.labels_\n",
        "\n",
        "# Analyze and Visualize Clusters.\n",
        "print(\"--- Top Terms per Cluster ---\")\n",
        "order_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]\n",
        "terms = tfidf_vectorizer.get_feature_names_out()\n",
        "for i in range(num_clusters):\n",
        "    top_terms = [terms[ind] for ind in order_centroids[i, :10]]\n",
        "    print(f\"Cluster {i}: {', '.join(top_terms)}\")\n",
        "\n",
        "# 2D Visualization of Clusters using PCA.\n",
        "pca = PCA(n_components=2, random_state=42)\n",
        "coords = pca.fit_transform(tfidf_matrix.toarray())\n",
        "\n",
        "# Create DataFrame for plotting.\n",
        "plot_df = pd.DataFrame({'x': coords[:, 0], 'y': coords[:, 1], 'cluster': df_sample['cluster']})\n",
        "\n",
        "# Create meaningful labels for legend.\n",
        "# These labels are based on the top terms found in each cluster.\n",
        "cluster_map = {\n",
        "    0: 'Technical/Engineering',\n",
        "    1: 'Business/Sales',\n",
        "    2: 'Retail/Customer Service',\n",
        "    3: 'General/Administrative',\n",
        "    4: 'Healthcare'\n",
        "}\n",
        "plot_df['cluster_label'] = plot_df['cluster'].map(cluster_map)\n",
        "\n",
        "# Plot clusters.\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.scatterplot(x='x', y='y', hue='cluster_label', data=plot_df, palette='viridis', alpha=0.7, s=50)\n",
        "plt.title('2D Visualization of Job Description Clusters', fontsize=16)\n",
        "plt.xlabel('PCA Component 1')\n",
        "plt.ylabel('PCA Component 2')\n",
        "plt.legend(title='Job Category')\n",
        "plt.grid(True)\n",
        "\n",
        "plt.suptitle('Job Market Insights: Job Clustering', fontsize=22, y=1.02)\n",
        "plt.tight_layout(pad=3.0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6fuGK8gMbNjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Interactive Dashboard for Job Market Insights\n",
        "\n",
        "# Import necessary libraries for interactive plotting.\n",
        "import plotly.graph_objects as go\n",
        "from ipywidgets import Dropdown, VBox\n",
        "\n",
        "# Prepare Data for the Dashboard.\n",
        "# Use the main 'postings_full' DataFrame.\n",
        "\n",
        "# Define comprehensive map of US state abbreviations to full names.\n",
        "state_map = {\n",
        "    'AL': 'Alabama', 'AK': 'Alaska', 'AZ': 'Arizona', 'AR': 'Arkansas', 'CA': 'California',\n",
        "    'CO': 'Colorado', 'CT': 'Connecticut', 'DE': 'Delaware', 'FL': 'Florida', 'GA': 'Georgia',\n",
        "    'HI': 'Hawaii', 'ID': 'Idaho', 'IL': 'Illinois', 'IN': 'Indiana', 'IA': 'Iowa',\n",
        "    'KS': 'Kansas', 'KY': 'Kentucky', 'LA': 'Louisiana', 'ME': 'Maine', 'MD': 'Maryland',\n",
        "    'MA': 'Massachusetts', 'MI': 'Michigan', 'MN': 'Minnesota', 'MS': 'Mississippi',\n",
        "    'MO': 'Missouri', 'MT': 'Montana', 'NE': 'Nebraska', 'NV': 'Nevada', 'NH': 'New Hampshire',\n",
        "    'NJ': 'New Jersey', 'NM': 'New Mexico', 'NY': 'New York', 'NC': 'North Carolina',\n",
        "    'ND': 'North Dakota', 'OH': 'Ohio', 'OK': 'Oklahoma', 'OR': 'Oregon', 'PA': 'Pennsylvania',\n",
        "    'RI': 'Rhode Island', 'SC': 'South Carolina', 'SD': 'South Dakota', 'TN': 'Tennessee',\n",
        "    'TX': 'Texas', 'UT': 'Utah', 'VT': 'Vermont', 'VA': 'Virginia', 'WA': 'Washington',\n",
        "    'WV': 'West Virginia', 'WI': 'Wisconsin', 'WY': 'Wyoming',\n",
        "    'DC': 'District of Columbia', 'PR': 'Puerto Rico'\n",
        "}\n",
        "valid_states = list(state_map.keys())\n",
        "\n",
        "# Filter unique states in data to include only valid abbreviations.\n",
        "actual_states_in_data = postings_full[postings_full['state'].isin(valid_states)]['state'].dropna().unique()\n",
        "\n",
        "# Create list for dropdown, including 'All States'.\n",
        "state_list = ['All States'] + sorted(list(actual_states_in_data))\n",
        "\n",
        "# Create Initial Figures as FigureWidgets.\n",
        "# Chart 1: Median Salary by Experience Level.\n",
        "initial_salary_data = postings_full.groupby('formatted_experience_level')['annual_salary'].median().dropna().sort_values()\n",
        "fig1 = go.FigureWidget(go.Bar(\n",
        "    x=initial_salary_data.values,\n",
        "    y=initial_salary_data.index,\n",
        "    orientation='h',\n",
        "    marker_color='purple'\n",
        "))\n",
        "fig1.update_layout(\n",
        "    title_text='Median Salary by Experience Level in All States',\n",
        "    xaxis_title='Median Annual Salary (USD)',\n",
        "    yaxis_title='Experience Level',\n",
        "    height=400\n",
        ")\n",
        "\n",
        "# Chart 2: Top Industries.\n",
        "initial_industry_data = postings_full['industries_list'].str.split(', ').explode().value_counts().nlargest(10)\n",
        "fig2 = go.FigureWidget(go.Bar(\n",
        "    x=initial_industry_data.values,\n",
        "    y=initial_industry_data.index,\n",
        "    orientation='h',\n",
        "    marker_color='orange'\n",
        "))\n",
        "fig2.update_layout(\n",
        "    title_text='Top 10 Industries in All States',\n",
        "    xaxis_title='Number of Postings',\n",
        "    yaxis_title='Industry',\n",
        "    yaxis={'autorange': 'reversed'},\n",
        "    height=400\n",
        ")\n",
        "\n",
        "# Create the Dropdown Widget.\n",
        "state_dropdown = Dropdown(options=state_list, description='Select State:')\n",
        "\n",
        "# Define the Function to Update the Charts.\n",
        "def update_charts(change):\n",
        "    selected_state = change.new\n",
        "\n",
        "    # Filter data based on selection.\n",
        "    if selected_state == 'All States':\n",
        "        df_filtered = postings_full\n",
        "    else:\n",
        "        df_filtered = postings_full[postings_full['state'] == selected_state]\n",
        "\n",
        "    # Calculate new data.\n",
        "    new_salary_data = df_filtered.groupby('formatted_experience_level')['annual_salary'].median().dropna().sort_values()\n",
        "    new_industry_data = df_filtered['industries_list'].str.split(', ').explode().value_counts().nlargest(10)\n",
        "\n",
        "    # Update charts using batch update.\n",
        "    with fig1.batch_update(), fig2.batch_update():\n",
        "        fig1.data[0].x = new_salary_data.values\n",
        "        fig1.data[0].y = new_salary_data.index\n",
        "        fig1.layout.title.text = f'Median Salary by Experience Level in {selected_state}'\n",
        "\n",
        "        fig2.data[0].x = new_industry_data.values\n",
        "        fig2.data[0].y = new_industry_data.index\n",
        "        fig2.layout.title.text = f'Top 10 Industries in {selected_state}'\n",
        "\n",
        "# Link the Dropdown to the Update Function.\n",
        "state_dropdown.observe(update_charts, names='value')\n",
        "\n",
        "# Display the Dashboard.\n",
        "VBox([state_dropdown, fig1, fig2])"
      ],
      "metadata": {
        "id": "fMsYLCKdfuF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Geospatial Analysis - Interactive Map of Job Postings\n",
        "\n",
        "# Import Plotly Express for creating the map.\n",
        "import plotly.express as px\n",
        "\n",
        "# Prepare Data for Mapping.\n",
        "# Filter out non-state entries.\n",
        "us_states_only = postings_full[postings_full['state'].isin(state_map.keys())] # Use state_map keys for filtering\n",
        "state_counts = us_states_only['state'].value_counts().reset_index()\n",
        "state_counts.columns = ['state_abbr', 'job_count']\n",
        "\n",
        "# Get full state names for hover text.\n",
        "state_counts['state_full'] = state_counts['state_abbr'].map(state_map)\n",
        "\n",
        "# Create the Choropleth Map.\n",
        "fig = px.choropleth(\n",
        "    state_counts,\n",
        "    locations='state_abbr',\n",
        "    locationmode=\"USA-states\",\n",
        "    color='job_count',\n",
        "    hover_name='state_full',\n",
        "    color_continuous_scale=\"Viridis\",\n",
        "    scope=\"usa\"\n",
        ")\n",
        "\n",
        "# Update Layout and Display.\n",
        "fig.update_layout(\n",
        "    title_text='Density of Job Postings Across the United States',\n",
        "    geo_bgcolor='rgba(0,0,0,0)',\n",
        "    paper_bgcolor='rgba(0,0,0,0)',\n",
        "    coloraxis_colorbar=dict(\n",
        "        title=\"Job Count\",\n",
        "        tickprefix=\"\",\n",
        "        ticks=\"outside\",\n",
        "        tickvals=[0, 2000, 4000, 6000, 8000, 10000, 12000],\n",
        "        ticktext=['0', '2k', '4k', '6k', '8k', '10k', '12k']\n",
        "    )\n",
        ")\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "ydKwOBsCye0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Advanced Visualization - Salary Distribution by Experience Level\n",
        "\n",
        "# Data Preparation.\n",
        "df_salary_dist = postings_full[['formatted_experience_level', 'annual_salary']].dropna()\n",
        "\n",
        "# Define logical order for experience levels.\n",
        "experience_order = [\n",
        "    'Internship',\n",
        "    'Entry level',\n",
        "    'Associate',\n",
        "    'Mid-Senior level',\n",
        "    'Director',\n",
        "    'Executive'\n",
        "]\n",
        "df_salary_dist = df_salary_dist[df_salary_dist['formatted_experience_level'].isin(experience_order)]\n",
        "\n",
        "# Create the Box Plot.\n",
        "plt.figure(figsize=(14, 8))\n",
        "sns.boxplot(\n",
        "    x='formatted_experience_level',\n",
        "    y='annual_salary',\n",
        "    data=df_salary_dist,\n",
        "    order=experience_order,\n",
        "    palette='magma'\n",
        ")\n",
        "\n",
        "# Formatting and Labels.\n",
        "plt.title('Annual Salary Distribution by Experience Level', fontsize=16)\n",
        "plt.xlabel('Experience Level', fontsize=12)\n",
        "plt.ylabel('Annual Salary (USD)', fontsize=12)\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Set y-axis limit to exclude extreme outliers.\n",
        "quantile_98 = postings_full['annual_salary'].quantile(0.98)\n",
        "plt.ylim(0, quantile_98)\n",
        "\n",
        "# Format y-axis to show currency with 'k'.\n",
        "plt.gca().yaxis.set_major_formatter(FuncFormatter(lambda y, pos: f'${y/1000:,.0f}k'))\n",
        "\n",
        "plt.suptitle('Job Market Insights: Salary Variation', fontsize=22, y=1.02)\n",
        "plt.tight_layout(pad=3.0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0U2RctvVzrsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Advanced Visualization - Heatmap of Job Postings by Industry and Experience Level\n",
        "\n",
        "# Data Preparation.\n",
        "df_heatmap = postings_full[['industries_list', 'formatted_experience_level']].dropna()\n",
        "\n",
        "# Split comma-separated industries.\n",
        "df_heatmap = df_heatmap.assign(industries_list=df_heatmap['industries_list'].str.split(', ')).explode('industries_list')\n",
        "\n",
        "# Focus on top 10 industries and main experience levels.\n",
        "top_10_industries = df_heatmap['industries_list'].value_counts().nlargest(10).index\n",
        "experience_order = ['Entry level', 'Associate', 'Mid-Senior level', 'Director', 'Executive']\n",
        "\n",
        "df_heatmap_filtered = df_heatmap[\n",
        "    df_heatmap['industries_list'].isin(top_10_industries) &\n",
        "    df_heatmap['formatted_experience_level'].isin(experience_order)\n",
        "]\n",
        "\n",
        "# Create a Crosstab.\n",
        "crosstab_df = pd.crosstab(\n",
        "    index=df_heatmap_filtered['industries_list'],\n",
        "    columns=df_heatmap_filtered['formatted_experience_level']\n",
        ")\n",
        "\n",
        "# Reorder columns by experience level.\n",
        "crosstab_df = crosstab_df[experience_order]\n",
        "\n",
        "# Create the Heatmap.\n",
        "plt.figure(figsize=(14, 10))\n",
        "sns.heatmap(\n",
        "    crosstab_df,\n",
        "    annot=True,\n",
        "    fmt='d',\n",
        "    cmap='YlGnBu',\n",
        "    linewidths=.5\n",
        ")\n",
        "\n",
        "# Formatting and Labels.\n",
        "plt.title('Concentration of Job Postings by Industry and Experience Level', fontsize=16)\n",
        "plt.xlabel('Experience Level', fontsize=12)\n",
        "plt.ylabel('Industry', fontsize=12)\n",
        "plt.xticks(rotation=45)\n",
        "plt.yticks(rotation=0)\n",
        "\n",
        "plt.suptitle('Job Market Insights: Industry Demand Matrix', fontsize=22, y=1.02)\n",
        "plt.tight_layout(pad=3.0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "P05OTAdK3-FV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analysis of Compensation Pay Periods by Top Job Roles\n",
        "\n",
        "# Data Preparation.\n",
        "df_job_pay = postings_full[['title', 'pay_period']].dropna()\n",
        "\n",
        "# Identify top 10 job titles.\n",
        "top_10_titles = df_job_pay['title'].value_counts().nlargest(10).index\n",
        "\n",
        "# Filter for top 10 job titles.\n",
        "df_top_jobs_pay = df_job_pay[df_job_pay['title'].isin(top_10_titles)]\n",
        "\n",
        "# Create a Crosstab of pay periods by job title.\n",
        "pay_period_crosstab = pd.crosstab(\n",
        "    index=df_top_jobs_pay['title'],\n",
        "    columns=df_top_jobs_pay['pay_period']\n",
        ")\n",
        "\n",
        "# Create the Stacked Bar Chart.\n",
        "pay_period_crosstab.plot(\n",
        "    kind='barh',\n",
        "    stacked=True,\n",
        "    figsize=(12, 8),\n",
        "    colormap='viridis'\n",
        ")\n",
        "\n",
        "# Formatting and Labels.\n",
        "plt.title('Pay Period Breakdown for Top 10 Job Titles', fontsize=16)\n",
        "plt.xlabel('Number of Job Postings', fontsize=12)\n",
        "plt.ylabel('Job Title', fontsize=12)\n",
        "plt.legend(title='Pay Period')\n",
        "\n",
        "# Format x-axis with 'k' formatter.\n",
        "plt.gca().xaxis.set_major_formatter(FuncFormatter(k_formatter))\n",
        "\n",
        "plt.suptitle('Job Market Insights: Compensation Structure by Role', fontsize=20, y=1.02)\n",
        "plt.tight_layout(pad=2.0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NJwWwOVF7zIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Comprehensive Interactive Dashboard by Job Role\n",
        "\n",
        "# Import necessary libraries.\n",
        "import plotly.graph_objects as go\n",
        "from ipywidgets import Dropdown, VBox, HBox\n",
        "\n",
        "# Prepare Data for the Dashboard.\n",
        "# Create list of top 50 job titles for dropdown.\n",
        "top_50_titles = postings_full['title'].value_counts().nlargest(50).index.tolist()\n",
        "job_title_list = ['All Roles'] + sorted(top_50_titles)\n",
        "\n",
        "# Add state map for converting abbreviations to full names.\n",
        "state_map = {\n",
        "    'AL': 'Alabama', 'AK': 'Alaska', 'AZ': 'Arizona', 'AR': 'Arkansas', 'CA': 'California',\n",
        "    'CO': 'Colorado', 'CT': 'Connecticut', 'DE': 'Delaware', 'FL': 'Florida', 'GA': 'Georgia',\n",
        "    'HI': 'Hawaii', 'ID': 'Idaho', 'IL': 'Illinois', 'IN': 'Indiana', 'IA': 'Iowa',\n",
        "    'KS': 'Kansas', 'KY': 'Kentucky', 'LA': 'Louisiana', 'ME': 'Maine', 'MD': 'Maryland',\n",
        "    'MA': 'Massachusetts', 'MI': 'Michigan', 'MN': 'Minnesota', 'MS': 'Mississippi',\n",
        "    'MO': 'Missouri', 'MT': 'Montana', 'NE': 'Nebraska', 'NV': 'Nevada', 'NH': 'New Hampshire',\n",
        "    'NJ': 'New Jersey', 'NM': 'New Mexico', 'NY': 'New York', 'NC': 'North Carolina',\n",
        "    'ND': 'North Dakota', 'OH': 'Ohio', 'OK': 'Oklahoma', 'OR': 'Oregon', 'PA': 'Pennsylvania',\n",
        "    'RI': 'Rhode Island', 'SC': 'South Carolina', 'SD': 'South Dakota', 'TN': 'Tennessee',\n",
        "    'TX': 'Texas', 'UT': 'Utah', 'VT': 'Vermont', 'VA': 'Virginia', 'WA': 'Washington',\n",
        "    'WV': 'West Virginia', 'WI': 'Wisconsin', 'WY': 'Wyoming',\n",
        "    'DC': 'District of Columbia', 'PR': 'Puerto Rico'\n",
        "}\n",
        "\n",
        "# Create Initial Figures as FigureWidgets.\n",
        "# Chart 1: Salary Distribution (Box Plot).\n",
        "fig_salary = go.FigureWidget(go.Box(\n",
        "    y=postings_full['annual_salary'].dropna(),\n",
        "    name='Salary Distribution',\n",
        "    marker_color='royalblue'\n",
        "))\n",
        "fig_salary.update_layout(\n",
        "    title_text='Salary Distribution for All Roles',\n",
        "    yaxis_title='Annual Salary (USD)',\n",
        "    margin=dict(l=20, r=20, t=50, b=20)\n",
        ")\n",
        "\n",
        "# Chart 2: Top States.\n",
        "top_states_initial = postings_full[postings_full['state'].isin(state_map.keys())]['state'].value_counts().nlargest(10) # Filter using keys\n",
        "top_states_initial.index = [state_map.get(abbr, abbr) for abbr in top_states_initial.index]\n",
        "fig_states = go.FigureWidget(go.Bar(\n",
        "    x=top_states_initial.values,\n",
        "    y=top_states_initial.index,\n",
        "    orientation='h',\n",
        "    marker_color='crimson'\n",
        "))\n",
        "fig_states.update_layout(\n",
        "    title_text='Top 10 States for All Roles',\n",
        "    yaxis={'autorange': 'reversed'},\n",
        "    margin=dict(l=150, r=20, t=50, b=50)\n",
        ")\n",
        "\n",
        "# Chart 3: Top Companies.\n",
        "top_companies_initial = postings_full[postings_full['company_name'] != 'Unknown Company']['company_name'].value_counts().nlargest(10)\n",
        "fig_companies = go.FigureWidget(go.Bar(\n",
        "    x=top_companies_initial.values,\n",
        "    y=top_companies_initial.index,\n",
        "    orientation='h',\n",
        "    marker_color='green'\n",
        "))\n",
        "fig_companies.update_layout(\n",
        "    title_text='Top 10 Companies for All Roles',\n",
        "    yaxis={'autorange': 'reversed'},\n",
        "    margin=dict(l=150, r=20, t=50, b=50)\n",
        ")\n",
        "\n",
        "# Chart 4: Top Skills.\n",
        "top_skills_initial = postings_full[postings_full['skills_list'] != 'Not specified']['skills_list'].str.split(', ').explode().value_counts().nlargest(10)\n",
        "fig_skills = go.FigureWidget(go.Bar(\n",
        "    x=top_skills_initial.values,\n",
        "    y=top_skills_initial.index,\n",
        "    orientation='h',\n",
        "    marker_color='goldenrod'\n",
        "))\n",
        "fig_skills.update_layout(\n",
        "    title_text='Top 10 Skills for All Roles',\n",
        "    yaxis={'autorange': 'reversed'},\n",
        "    margin=dict(l=150, r=20, t=50, b=50)\n",
        ")\n",
        "\n",
        "# Create the Dropdown Widget.\n",
        "job_dropdown = Dropdown(options=job_title_list, description='Select Job Role:')\n",
        "\n",
        "# Define the Update Function.\n",
        "def update_dashboard(change):\n",
        "    selected_role = change.new\n",
        "\n",
        "    # Filter data based on selection.\n",
        "    if selected_role == 'All Roles':\n",
        "        df_filtered = postings_full\n",
        "    else:\n",
        "        df_filtered = postings_full[postings_full['title'] == selected_role]\n",
        "\n",
        "    # Calculate new data for each chart.\n",
        "    new_salary_data = df_filtered['annual_salary'].dropna()\n",
        "    new_states_data = df_filtered[df_filtered['state'].isin(state_map.keys())]['state'].value_counts().nlargest(10) # Filter using keys\n",
        "    new_states_data.index = [state_map.get(abbr, abbr) for abbr in new_states_data.index]\n",
        "    new_companies_data = df_filtered[df_filtered['company_name'] != 'Unknown Company']['company_name'].value_counts().nlargest(10)\n",
        "    new_skills_data = df_filtered[df_filtered['skills_list'] != 'Not specified']['skills_list'].str.split(', ').explode().value_counts().nlargest(10)\n",
        "\n",
        "    # Update charts using batch_update.\n",
        "    with fig_salary.batch_update(), fig_states.batch_update(), fig_companies.batch_update(), fig_skills.batch_update():\n",
        "        fig_salary.data[0].y = new_salary_data\n",
        "        fig_salary.layout.title.text = f'Salary Distribution for {selected_role}'\n",
        "\n",
        "        fig_states.data[0].x = new_states_data.values\n",
        "        fig_states.data[0].y = new_states_data.index\n",
        "        fig_states.layout.title.text = f'Top 10 States for {selected_role}'\n",
        "\n",
        "        fig_companies.data[0].x = new_companies_data.values\n",
        "        fig_companies.data[0].y = new_companies_data.index\n",
        "        fig_companies.layout.title.text = f'Top 10 Companies for {selected_role}'\n",
        "\n",
        "        fig_skills.data[0].x = new_skills_data.values\n",
        "        fig_skills.data[0].y = new_skills_data.index\n",
        "        fig_skills.layout.title.text = f'Top 10 Skills for {selected_role}'\n",
        "\n",
        "# Link Dropdown to Update Function.\n",
        "job_dropdown.observe(update_dashboard, names='value')\n",
        "\n",
        "# Display the Dashboard.\n",
        "dashboard = VBox([\n",
        "    job_dropdown,\n",
        "    HBox([fig_salary, fig_states]),\n",
        "    HBox([fig_companies, fig_skills])\n",
        "])\n",
        "dashboard"
      ],
      "metadata": {
        "id": "vg0AmAQH879i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce8be3a2"
      },
      "source": [
        "## Key Insights from the LinkedIn Job Postings Analysis\n",
        "\n",
        "Based on the exploratory data analysis, natural language processing, and machine learning performed on the LinkedIn job postings dataset, here are the key insights into the digital job market:\n",
        "\n",
        "### Overall Job Market Overview\n",
        "\n",
        "*   **Experience Level Distribution:** The majority of job postings are for **Mid-Senior level** and **Entry level** positions, indicating a strong demand for both experienced professionals and those starting their careers.\n",
        "*   **Top Job Titles:** The most common job titles reflect a diverse market, with roles like **Sales Manager**, **Customer Service Representative**, and **Project Manager** appearing frequently.\n",
        "*   **Employment Type:** **Full-time** positions dominate the job market, although there are also opportunities for contract, part-time, and temporary roles.\n",
        "*   **Remote Work:** A significant portion of job postings **do not allow remote work**, with only a smaller percentage explicitly indicating remote options.\n",
        "\n",
        "### Industry and Skills Analysis\n",
        "\n",
        "*   **Top Industries:** The **Hospitals and Health Care**, **Retail**, and **IT Services and IT Consulting** sectors have the highest number of job postings, suggesting robust hiring in these areas.\n",
        "*   **In-Demand Skills:** The word cloud highlights key skills sought by employers, including **Information Technology**, **Sales**, **Management**, **Manufacturing**, and **Health Care Provider**.\n",
        "\n",
        "### Salary Analysis\n",
        "\n",
        "*   **Salary Distribution:** The distribution of annual salaries shows a wide range, with a concentration of postings in the **\\$50k to \\$100k** range.\n",
        "*   **Salary by Experience Level:** As expected, there is a clear correlation between experience level and median annual salary, with **Executive** and **Director** level positions commanding the highest median salaries. **Internship** and **Entry level** positions have the lowest median salaries.\n",
        "*   **Salary by Company Size:** The median annual salary generally **increases with company size**, with companies having over 10,000 employees offering the highest median salaries.\n",
        "\n",
        "### Geographical Analysis\n",
        "\n",
        "*   **Top States for Job Postings:** **California**, **Texas**, and **New York** lead the United States in the number of LinkedIn job postings, indicating these states are major job markets.\n",
        "\n",
        "### Natural Language Processing Insights\n",
        "\n",
        "*   **Common Phrases in Job Descriptions:** Analysis of job descriptions reveals frequently used phrases related to **equal opportunity**, **experience requirements**, **customer service**, and **teamwork**. This provides insight into common hiring criteria and workplace expectations.\n",
        "\n",
        "### Job Clustering\n",
        "\n",
        "*   **Job Categories:** Clustering of job descriptions reveals distinct categories such as **Technical/Engineering**, **Business/Sales**, **Retail/Customer Service**, **General/Administrative**, and **Healthcare**, indicating specialization within the job market.\n",
        "\n",
        "### Compensation Structure\n",
        "\n",
        "*   **Pay Period Breakdown:** The analysis of pay periods for top job titles shows that **YEARLY** salaries are the most common, followed by **HOURLY** wages.\n",
        "\n",
        "### Machine Learning Model (Salary Prediction)\n",
        "\n",
        "*   **Feature Importance:** The Machine Learning model for predicting annual salary highlights that **Experience Level** is the most important factor, followed by **Company Size** and **State**.\n",
        "\n",
        "These insights provide a comprehensive overview of the LinkedIn job market, highlighting trends in demand, compensation, location, and key skills."
      ]
    }
  ]
}